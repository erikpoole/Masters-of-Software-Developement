{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports and setup \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, KFold\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_pixel_values(raw_df):\n",
    "    df = raw_df.copy();\n",
    "    for i in range(12):\n",
    "        df.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "\n",
    "    ret = []\n",
    "    column_vals = df.columns\n",
    "    df_matrix = df.to_numpy()\n",
    "    \n",
    "    for row in range(len(df_matrix)):\n",
    "        pixel_matrix = np.zeros((20, 20))\n",
    "        for col in range(len(df_matrix[0])):\n",
    "            header = column_vals[col]\n",
    "            splitHeader = header[1:].split('c')\n",
    "            pixelRow = splitHeader[0]\n",
    "            pixelCol = splitHeader[1]\n",
    "            pixel_matrix[int(pixelRow)][int(pixelCol)] = df_matrix[row][col] / 255.0\n",
    "        ret.append(pixel_matrix)\n",
    "    \n",
    "    ret = np.stack(ret)\n",
    "    ret = np.reshape(x, (-1, 20, 20, 1))\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maps_from_labels(input_arr):  \n",
    "    val_to_ix = { val:i for i,val in enumerate(np.unique(y_raw)) }\n",
    "    ix_to_val = { i:val for i,val in enumerate(np.unique(y_raw)) }\n",
    "    \n",
    "    return val_to_ix, ix_to_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class font:\n",
    "    df\n",
    "    x\n",
    "    y\n",
    "    unique_char_count\n",
    "    val_to_ix\n",
    "    val_to_char\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        self.df = pd.read_csv('fonts/ARIAL.csv')\n",
    "        y_raw = np.array(df.array)\n",
    "        self.val_to_ix, self.val_to_char = get_maps_from_lables(y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "arial = font('fonts/ARIAL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_raw = np.array(df.m_label)\n",
    "x = organize_pixel_values(df)\n",
    "\n",
    "\n",
    "# print(len(y_raw))\n",
    "# print(len(x))\n",
    "# print(len(x[0]))\n",
    "# print(len(x[0][0]))\n",
    "# print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_to_ix, val_to_char = get_maps_from_labels(y_raw)\n",
    "\n",
    "# print(ix_to_char[32])\n",
    "# print(char_to_ix[64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_char_count = len(val_to_ix)\n",
    "\n",
    "y = []\n",
    "for val in y_raw:\n",
    "    arr = np.zeros(unique_char_count)\n",
    "    arr[val_to_ix[val]] = 1;\n",
    "    y.append(arr)\n",
    "\n",
    "y = np.array(y)\n",
    "\n",
    "# print(unique_char_count)\n",
    "# print(len(y_raw))\n",
    "# print(len(y[0]))\n",
    "# print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0710 14:51:14.895457 4669994432 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0710 14:51:14.909225 4669994432 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0710 14:51:14.911177 4669994432 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0710 14:51:14.923674 4669994432 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0710 14:51:14.943591 4669994432 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0710 14:51:14.949818 4669994432 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(20, 20, 1)))\n",
    "model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0710 14:51:14.984170 4669994432 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0710 14:51:15.005231 4669994432 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26237, 20, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_reshaped, y, random_state=1, test_size=0.8)\n",
    "\n",
    "print(x_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5247 samples, validate on 20990 samples\n",
      "Epoch 1/20\n",
      "5247/5247 [==============================] - 5s 992us/step - loss: 3.4045 - acc: 0.3848 - val_loss: 5.5738 - val_acc: 0.3579\n",
      "Epoch 2/20\n",
      "5247/5247 [==============================] - 5s 924us/step - loss: 2.5925 - acc: 0.4513 - val_loss: 5.5182 - val_acc: 0.3870\n",
      "Epoch 3/20\n",
      "5247/5247 [==============================] - 5s 932us/step - loss: 2.0328 - acc: 0.5079 - val_loss: 5.3663 - val_acc: 0.4065\n",
      "Epoch 4/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 1.6738 - acc: 0.5573 - val_loss: 5.3792 - val_acc: 0.4177\n",
      "Epoch 5/20\n",
      "5247/5247 [==============================] - 5s 912us/step - loss: 1.4120 - acc: 0.6003 - val_loss: 5.3945 - val_acc: 0.4240\n",
      "Epoch 6/20\n",
      "5247/5247 [==============================] - 4s 851us/step - loss: 1.2449 - acc: 0.6371 - val_loss: 5.3341 - val_acc: 0.4316\n",
      "Epoch 7/20\n",
      "5247/5247 [==============================] - 5s 867us/step - loss: 1.1088 - acc: 0.6745 - val_loss: 5.3448 - val_acc: 0.4398\n",
      "Epoch 8/20\n",
      "5247/5247 [==============================] - 4s 845us/step - loss: 1.0104 - acc: 0.6974 - val_loss: 5.2878 - val_acc: 0.4470\n",
      "Epoch 9/20\n",
      "5247/5247 [==============================] - 5s 865us/step - loss: 0.9334 - acc: 0.7134 - val_loss: 5.3565 - val_acc: 0.4364\n",
      "Epoch 10/20\n",
      "5247/5247 [==============================] - 4s 853us/step - loss: 0.8747 - acc: 0.7294 - val_loss: 5.3589 - val_acc: 0.4494\n",
      "Epoch 11/20\n",
      "5247/5247 [==============================] - 5s 903us/step - loss: 0.8056 - acc: 0.7482 - val_loss: 5.3950 - val_acc: 0.4535\n",
      "Epoch 12/20\n",
      "5247/5247 [==============================] - 5s 888us/step - loss: 0.7829 - acc: 0.7505 - val_loss: 5.4332 - val_acc: 0.4495\n",
      "Epoch 13/20\n",
      "5247/5247 [==============================] - 5s 905us/step - loss: 0.7366 - acc: 0.7658 - val_loss: 5.4119 - val_acc: 0.4552\n",
      "Epoch 14/20\n",
      "5247/5247 [==============================] - 5s 888us/step - loss: 0.7231 - acc: 0.7644 - val_loss: 5.5023 - val_acc: 0.4555\n",
      "Epoch 15/20\n",
      "5247/5247 [==============================] - 5s 904us/step - loss: 0.6630 - acc: 0.7888 - val_loss: 5.4342 - val_acc: 0.4548\n",
      "Epoch 16/20\n",
      "5247/5247 [==============================] - 5s 901us/step - loss: 0.6431 - acc: 0.7843 - val_loss: 5.4428 - val_acc: 0.4584\n",
      "Epoch 17/20\n",
      "5247/5247 [==============================] - 5s 864us/step - loss: 0.6086 - acc: 0.7995 - val_loss: 5.5227 - val_acc: 0.4598\n",
      "Epoch 18/20\n",
      "5247/5247 [==============================] - 5s 865us/step - loss: 0.6042 - acc: 0.7987 - val_loss: 5.4918 - val_acc: 0.4605\n",
      "Epoch 19/20\n",
      "5247/5247 [==============================] - 5s 900us/step - loss: 0.5689 - acc: 0.8104 - val_loss: 5.5448 - val_acc: 0.4555\n",
      "Epoch 20/20\n",
      "5247/5247 [==============================] - 5s 881us/step - loss: 0.5636 - acc: 0.8159 - val_loss: 5.6319 - val_acc: 0.4572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13fb10828>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the network using cross validation (splitting data into training/testing). What is its accuracy? ###\n",
    "\n",
    "**Training and testing on Arial, our model gives us an accuracy of around 82% after 20 epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Conv2D(128, kernel_size=3, activation='relu', input_shape=(20, 20, 1)))\n",
    "model2.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model2.add(Conv2D(128, kernel_size=3, activation='relu'))\n",
    "model2.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dropout(.1))\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dense(unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5247 samples, validate on 20990 samples\n",
      "Epoch 1/20\n",
      "5247/5247 [==============================] - 11s 2ms/step - loss: 6.4465 - acc: 0.1896 - val_loss: 5.7674 - val_acc: 0.2899\n",
      "Epoch 2/20\n",
      "5247/5247 [==============================] - 12s 2ms/step - loss: 4.8165 - acc: 0.3112 - val_loss: 5.3951 - val_acc: 0.3081\n",
      "Epoch 3/20\n",
      "5247/5247 [==============================] - 12s 2ms/step - loss: 3.6847 - acc: 0.3751 - val_loss: 5.4963 - val_acc: 0.3708\n",
      "Epoch 4/20\n",
      "5247/5247 [==============================] - 12s 2ms/step - loss: 2.6442 - acc: 0.4667 - val_loss: 5.2158 - val_acc: 0.4047\n",
      "Epoch 5/20\n",
      "5247/5247 [==============================] - 12s 2ms/step - loss: 1.9120 - acc: 0.5333 - val_loss: 5.2117 - val_acc: 0.4180\n",
      "Epoch 6/20\n",
      "5247/5247 [==============================] - 13s 2ms/step - loss: 1.4833 - acc: 0.5939 - val_loss: 5.1709 - val_acc: 0.4340\n",
      "Epoch 7/20\n",
      "5247/5247 [==============================] - 12s 2ms/step - loss: 1.2221 - acc: 0.6468 - val_loss: 5.1147 - val_acc: 0.4437\n",
      "Epoch 8/20\n",
      "5247/5247 [==============================] - 12s 2ms/step - loss: 1.0511 - acc: 0.6861 - val_loss: 5.1236 - val_acc: 0.4529\n",
      "Epoch 9/20\n",
      "5247/5247 [==============================] - 10s 2ms/step - loss: 0.9512 - acc: 0.7092 - val_loss: 5.1927 - val_acc: 0.4567\n",
      "Epoch 10/20\n",
      "5247/5247 [==============================] - 10s 2ms/step - loss: 0.8672 - acc: 0.7336 - val_loss: 5.1474 - val_acc: 0.4632\n",
      "Epoch 11/20\n",
      "5247/5247 [==============================] - 11s 2ms/step - loss: 0.7875 - acc: 0.7486 - val_loss: 5.1769 - val_acc: 0.4636\n",
      "Epoch 12/20\n",
      "5247/5247 [==============================] - 13s 2ms/step - loss: 0.7312 - acc: 0.7608 - val_loss: 5.2290 - val_acc: 0.4643\n",
      "Epoch 13/20\n",
      "5247/5247 [==============================] - 12s 2ms/step - loss: 0.6931 - acc: 0.7766 - val_loss: 5.2203 - val_acc: 0.4672\n",
      "Epoch 14/20\n",
      "5247/5247 [==============================] - 13s 2ms/step - loss: 0.6552 - acc: 0.7902 - val_loss: 5.3054 - val_acc: 0.4677\n",
      "Epoch 15/20\n",
      "5247/5247 [==============================] - 13s 2ms/step - loss: 0.6187 - acc: 0.7982 - val_loss: 5.2078 - val_acc: 0.4680\n",
      "Epoch 16/20\n",
      "5247/5247 [==============================] - 14s 3ms/step - loss: 0.5897 - acc: 0.8081 - val_loss: 5.2971 - val_acc: 0.4723\n",
      "Epoch 17/20\n",
      "5247/5247 [==============================] - 13s 2ms/step - loss: 0.5651 - acc: 0.8193 - val_loss: 5.3134 - val_acc: 0.4716\n",
      "Epoch 18/20\n",
      "5247/5247 [==============================] - 12s 2ms/step - loss: 0.5264 - acc: 0.8266 - val_loss: 5.2915 - val_acc: 0.4617\n",
      "Epoch 19/20\n",
      "5247/5247 [==============================] - 10s 2ms/step - loss: 0.5234 - acc: 0.8302 - val_loss: 5.2514 - val_acc: 0.4695\n",
      "Epoch 20/20\n",
      "5247/5247 [==============================] - 10s 2ms/step - loss: 0.4851 - acc: 0.8391 - val_loss: 5.3721 - val_acc: 0.4738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1389af5c0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train a different network topology (add more convolution/dropout layers, explore other types/sizes of layer). Try to find a topology that works better than the one described above. ###\n",
    "\n",
    "**I took a very simple approach with my second model, I just doubled the number of perceptrons of the convolutional and dense layers. This new model gives us a modest increase of accuracy (84% after 20 epochs).  Each epoch took between 2 and 2.5x as long to run though, so it's possible that the increased accuracy would not be worth the time/computing cost in practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Conv2D(64, kernel_size=5, activation='relu', input_shape=(20, 20, 1)))\n",
    "model3.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model3.add(Conv2D(64, kernel_size=5, activation='relu'))\n",
    "model3.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dropout(.1))\n",
    "model3.add(Dense(64, activation='relu'))\n",
    "model3.add(Dense(unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5247 samples, validate on 20990 samples\n",
      "Epoch 1/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 6.6763 - acc: 0.1498 - val_loss: 5.9241 - val_acc: 0.2391\n",
      "Epoch 2/20\n",
      "5247/5247 [==============================] - 5s 972us/step - loss: 5.0699 - acc: 0.2914 - val_loss: 5.5811 - val_acc: 0.2917\n",
      "Epoch 3/20\n",
      "5247/5247 [==============================] - 5s 941us/step - loss: 4.1209 - acc: 0.3276 - val_loss: 5.5674 - val_acc: 0.3326\n",
      "Epoch 4/20\n",
      "5247/5247 [==============================] - 5s 887us/step - loss: 3.2560 - acc: 0.4006 - val_loss: 5.4225 - val_acc: 0.3732\n",
      "Epoch 5/20\n",
      "5247/5247 [==============================] - 5s 880us/step - loss: 2.5727 - acc: 0.4490 - val_loss: 5.3024 - val_acc: 0.3965\n",
      "Epoch 6/20\n",
      "5247/5247 [==============================] - 5s 873us/step - loss: 2.0823 - acc: 0.4961 - val_loss: 5.1964 - val_acc: 0.4136\n",
      "Epoch 7/20\n",
      "5247/5247 [==============================] - 5s 879us/step - loss: 1.7761 - acc: 0.5312 - val_loss: 5.1609 - val_acc: 0.4232\n",
      "Epoch 8/20\n",
      "5247/5247 [==============================] - 5s 913us/step - loss: 1.5483 - acc: 0.5716 - val_loss: 5.1432 - val_acc: 0.4312\n",
      "Epoch 9/20\n",
      "5247/5247 [==============================] - 5s 906us/step - loss: 1.3957 - acc: 0.6061 - val_loss: 5.1160 - val_acc: 0.4403\n",
      "Epoch 10/20\n",
      "5247/5247 [==============================] - 5s 879us/step - loss: 1.2852 - acc: 0.6223 - val_loss: 5.1120 - val_acc: 0.4421\n",
      "Epoch 11/20\n",
      "5247/5247 [==============================] - 5s 887us/step - loss: 1.1869 - acc: 0.6497 - val_loss: 5.1068 - val_acc: 0.4529\n",
      "Epoch 12/20\n",
      "5247/5247 [==============================] - 5s 885us/step - loss: 1.0997 - acc: 0.6722 - val_loss: 5.1016 - val_acc: 0.4491\n",
      "Epoch 13/20\n",
      "5247/5247 [==============================] - 5s 896us/step - loss: 1.0467 - acc: 0.6821 - val_loss: 5.0875 - val_acc: 0.4578\n",
      "Epoch 14/20\n",
      "5247/5247 [==============================] - 5s 893us/step - loss: 0.9798 - acc: 0.6972 - val_loss: 5.1640 - val_acc: 0.4574\n",
      "Epoch 15/20\n",
      "5247/5247 [==============================] - 5s 900us/step - loss: 0.9171 - acc: 0.7139 - val_loss: 5.1464 - val_acc: 0.4593\n",
      "Epoch 16/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 0.9225 - acc: 0.7116 - val_loss: 5.1697 - val_acc: 0.4626\n",
      "Epoch 17/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 0.8738 - acc: 0.7240 - val_loss: 5.1642 - val_acc: 0.4623\n",
      "Epoch 18/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 0.8275 - acc: 0.7391 - val_loss: 5.1688 - val_acc: 0.4627\n",
      "Epoch 19/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 0.7977 - acc: 0.7524 - val_loss: 5.1659 - val_acc: 0.4657\n",
      "Epoch 20/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 0.7750 - acc: 0.7507 - val_loss: 5.1612 - val_acc: 0.4711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13e5172b0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "\n",
    "model4.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(20, 20, 1)))\n",
    "model4.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model4.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model4.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model4.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model4.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model4.add(Flatten())\n",
    "model4.add(Dropout(.1))\n",
    "model4.add(Dense(64, activation='relu'))\n",
    "model4.add(Dense(unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5247 samples, validate on 20990 samples\n",
      "Epoch 1/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 6.9158 - acc: 0.0667 - val_loss: 6.3631 - val_acc: 0.1815\n",
      "Epoch 2/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 5.6002 - acc: 0.2424 - val_loss: 5.7579 - val_acc: 0.2681\n",
      "Epoch 3/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 4.7324 - acc: 0.2977 - val_loss: 5.5887 - val_acc: 0.2925\n",
      "Epoch 4/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 4.0835 - acc: 0.3219 - val_loss: 5.5692 - val_acc: 0.3213\n",
      "Epoch 5/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 3.5417 - acc: 0.3570 - val_loss: 5.6112 - val_acc: 0.3432\n",
      "Epoch 6/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 3.0736 - acc: 0.3896 - val_loss: 5.5713 - val_acc: 0.3685\n",
      "Epoch 7/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 2.6713 - acc: 0.4187 - val_loss: 5.5341 - val_acc: 0.3766\n",
      "Epoch 8/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 2.3887 - acc: 0.4479 - val_loss: 5.4806 - val_acc: 0.3869\n",
      "Epoch 9/20\n",
      "5247/5247 [==============================] - 5s 1ms/step - loss: 2.1396 - acc: 0.4700 - val_loss: 5.4392 - val_acc: 0.3999\n",
      "Epoch 10/20\n",
      "5247/5247 [==============================] - 5s 967us/step - loss: 1.9816 - acc: 0.4930 - val_loss: 5.4253 - val_acc: 0.4048\n",
      "Epoch 11/20\n",
      "5247/5247 [==============================] - 5s 1ms/step - loss: 1.8413 - acc: 0.5167 - val_loss: 5.3866 - val_acc: 0.4057\n",
      "Epoch 12/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 1.7080 - acc: 0.5390 - val_loss: 5.3699 - val_acc: 0.4177\n",
      "Epoch 13/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 1.6332 - acc: 0.5617 - val_loss: 5.3574 - val_acc: 0.4219\n",
      "Epoch 14/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 1.5722 - acc: 0.5698 - val_loss: 5.3828 - val_acc: 0.4248\n",
      "Epoch 15/20\n",
      "5247/5247 [==============================] - 5s 1ms/step - loss: 1.4928 - acc: 0.5836 - val_loss: 5.3612 - val_acc: 0.4241\n",
      "Epoch 16/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 1.3993 - acc: 0.6099 - val_loss: 5.3825 - val_acc: 0.4323\n",
      "Epoch 17/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 1.3520 - acc: 0.6133 - val_loss: 5.3833 - val_acc: 0.4303\n",
      "Epoch 18/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 1.3122 - acc: 0.6266 - val_loss: 5.3710 - val_acc: 0.4329\n",
      "Epoch 19/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 1.2563 - acc: 0.6390 - val_loss: 5.2922 - val_acc: 0.4381\n",
      "Epoch 20/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 1.2418 - acc: 0.6402 - val_loss: 5.3071 - val_acc: 0.4381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17ab162e8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third and Fourth Models ###\n",
    "\n",
    "**I did a third and forth model mostly for my enjoyment (since the second model was already an improvment over the first).  The third model I changed the kernel size to 5x5 and it underpreformed with 75% accuracy.  The fourth model I added one additional convolutional layer * max pooling layer and (to my surprise) it dramatically underperformed at 64% accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('fonts/TIMES.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_raw2 = np.array(df2.m_label)\n",
    "x2 = organize_pixel_values(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_to_ix2, val_to_char2 = get_maps_from_labels(y_raw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_char_count2 = len(val_to_ix)\n",
    "\n",
    "y2 = []\n",
    "for val in y_raw2:\n",
    "    arr = np.zeros(unique_char_count)\n",
    "    arr[val_to_ix[val]] = 1;\n",
    "    y.append(arr)\n",
    "\n",
    "y = np.array(y)\n",
    "\n",
    "# print(unique_char_count)\n",
    "# print(len(y_raw))\n",
    "# print(len(y[0]))\n",
    "# print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create and train a different network topology (add more convolution/dropout layers, explore other types/sizes of layer). Try to find a topology that works better than the one described above.\n",
    "\n",
    "\n",
    "Test the accuracy of your network with character inputs from a DIFFERENT font set. How does it perform? (He means train on one font, test on another)\n",
    "\n",
    "Train your best network on inputs from the data from at least 2 different fonts. How does your accuracy compare to the 1-font case? What accuracy do you see when testing with inputs from a font you didn't train on?\n",
    "Take a look at some of the characters that have been misclassified. Do you notice any patterns? The network only produces the relative probabilities that the input is any of the possible characters. Can you find examples where the network is unsure of the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
