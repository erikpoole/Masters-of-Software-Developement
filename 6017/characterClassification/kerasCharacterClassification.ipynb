{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports and setup \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, KFold\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_pixel_values(raw_df):\n",
    "    df = raw_df.copy();\n",
    "    for i in range(12):\n",
    "        df.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "\n",
    "    ret = []\n",
    "    column_vals = df.columns\n",
    "    df_matrix = df.to_numpy()\n",
    "    \n",
    "    for row in range(len(df_matrix)):\n",
    "        pixel_matrix = np.zeros((20, 20))\n",
    "        for col in range(len(df_matrix[0])):\n",
    "            header = column_vals[col]\n",
    "            splitHeader = header[1:].split('c')\n",
    "            pixelRow = splitHeader[0]\n",
    "            pixelCol = splitHeader[1]\n",
    "            pixel_matrix[int(pixelRow)][int(pixelCol)] = df_matrix[row][col] / 255.0\n",
    "        ret.append(pixel_matrix)\n",
    "    \n",
    "    ret = np.stack(ret)\n",
    "    ret = np.reshape(ret, (-1, 20, 20, 1))\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maps_from_labels(input_arr):  \n",
    "    val_to_ix = { val:i for i,val in enumerate(np.unique(input_arr)) }\n",
    "    ix_to_val = { i:val for i,val in enumerate(np.unique(input_arr)) }\n",
    "    \n",
    "    return val_to_ix, ix_to_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconcileFonts(font1, font2):\n",
    "#     for i in font1.y\n",
    "#         if value is not key in font1.ix_to_val\n",
    "#             remove font1.y[i], font1.x[i]\n",
    "\n",
    "    font1_keys = set(font1.val_to_ix.keys())\n",
    "    font2_keys = set(font2.val_to_ix.keys())\n",
    "    \n",
    "    print(len(font1_keys), len(font2_keys))\n",
    "    \n",
    "    intersection = np.array(font1_keys & font2_keys)\n",
    "    \n",
    "    print(intersection.shape)\n",
    "    print()\n",
    "    print(font1.y_raw.shape)\n",
    "    \n",
    "#     font1.val_to_ix, font1.ix_to_val = get_maps_from_labels(np.array(intersection))\n",
    "#     font2.val_to_ix, font2.ix_to_val = get_maps_from_labels(np.array(intersection))\n",
    "    \n",
    "#     print(len(font1.val_to_ix), len(font2.val_to_ix))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     y_fixed = []\n",
    "#     x_fixed = []\n",
    "#     for i in range(len(font1.y_raw)):\n",
    "#         if (i in font2.ix_to_val):\n",
    "#             y_fixed.append(font1.y_raw[i])\n",
    "#             x_fixed.append(font1.x[i])\n",
    "#             print(\"Save\")\n",
    "#         else:\n",
    "#             print (\"Delete\")\n",
    "    \n",
    "#     print(y_fixed)\n",
    "#     print(get_maps_from_labels(y_fixed)[0])\n",
    "#     print(len(get_maps_from_labels(y_fixed)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3098 3087\n",
      "()\n",
      "\n",
      "(26237,)\n"
     ]
    }
   ],
   "source": [
    "reconcileFonts(arial, timesNewRoman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class font:\n",
    "    x = None\n",
    "    y = None\n",
    "    y_raw = None\n",
    "    \n",
    "    unique_char_count = 0\n",
    "    val_to_ix = None\n",
    "    ix_to_val = None\n",
    "    \n",
    "    def __init__(self, file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        self.x = organize_pixel_values(df)\n",
    "        \n",
    "        self.y_raw = np.array(df.m_label)\n",
    "        self.val_to_ix, self.ix_to_val = get_maps_from_labels(self.y_raw)\n",
    "        self.unique_char_count = len(self.val_to_ix)\n",
    "        \n",
    "        self.y = self.get_1_hot()\n",
    "        \n",
    "    def get_1_hot(self):\n",
    "        ret = []\n",
    "        for val in self.y_raw:\n",
    "            arr = np.zeros(self.unique_char_count)\n",
    "            arr[self.val_to_ix[val]] = 1;\n",
    "            ret.append(arr)\n",
    "\n",
    "        return np.array(ret)\n",
    "    \n",
    "    def display_attributes(self):\n",
    "        print(self.ix_to_val[0])\n",
    "        print(self.val_to_ix[33])\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(self.unique_char_count)\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(self.x.shape)\n",
    "        print(self.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "arial = font('fonts/ARIAL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "0\n",
      "\n",
      "3098\n",
      "\n",
      "(26237, 20, 20, 1)\n",
      "(26237, 3098)\n"
     ]
    }
   ],
   "source": [
    "arial.display_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(arial.x, arial.y, random_state=1, test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0711 14:43:23.030337 4476843456 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0711 14:43:23.076234 4476843456 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0711 14:43:23.092101 4476843456 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0711 14:43:23.126682 4476843456 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0711 14:43:23.165050 4476843456 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0711 14:43:23.172092 4476843456 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(20, 20, 1)))\n",
    "model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(arial.unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0711 14:43:23.223360 4476843456 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0711 14:43:23.250074 4476843456 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0711 14:43:23.408517 4476843456 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5247 samples, validate on 20990 samples\n",
      "Epoch 1/20\n",
      "5247/5247 [==============================] - 5s 1ms/step - loss: 6.6695 - acc: 0.1618 - val_loss: 5.9132 - val_acc: 0.2483\n",
      "Epoch 2/20\n",
      "5247/5247 [==============================] - 5s 942us/step - loss: 5.1363 - acc: 0.2929 - val_loss: 5.6574 - val_acc: 0.2922\n",
      "Epoch 3/20\n",
      "5247/5247 [==============================] - 5s 950us/step - loss: 4.2986 - acc: 0.3208 - val_loss: 5.7233 - val_acc: 0.3297\n",
      "Epoch 4/20\n",
      "5247/5247 [==============================] - 5s 938us/step - loss: 3.4136 - acc: 0.3932 - val_loss: 5.5520 - val_acc: 0.3733\n",
      "Epoch 5/20\n",
      "5247/5247 [==============================] - 5s 951us/step - loss: 2.6171 - acc: 0.4629 - val_loss: 5.3779 - val_acc: 0.3998\n",
      "Epoch 6/20\n",
      "5247/5247 [==============================] - 5s 948us/step - loss: 2.0303 - acc: 0.5186 - val_loss: 5.2678 - val_acc: 0.4192\n",
      "Epoch 7/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 1.6500 - acc: 0.5695 - val_loss: 5.2637 - val_acc: 0.4136\n",
      "Epoch 8/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 1.3812 - acc: 0.6190 - val_loss: 5.2006 - val_acc: 0.4333\n",
      "Epoch 9/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 1.2262 - acc: 0.6518 - val_loss: 5.1443 - val_acc: 0.4457\n",
      "Epoch 10/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 1.1057 - acc: 0.6752 - val_loss: 5.1511 - val_acc: 0.4509\n",
      "Epoch 11/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 0.9937 - acc: 0.6989 - val_loss: 5.1851 - val_acc: 0.4553\n",
      "Epoch 12/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 0.9391 - acc: 0.7115 - val_loss: 5.1566 - val_acc: 0.4586\n",
      "Epoch 13/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 0.8722 - acc: 0.7290 - val_loss: 5.1852 - val_acc: 0.4616\n",
      "Epoch 14/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 0.8069 - acc: 0.7439 - val_loss: 5.1780 - val_acc: 0.4672\n",
      "Epoch 15/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 0.7571 - acc: 0.7612 - val_loss: 5.2137 - val_acc: 0.4626\n",
      "Epoch 16/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 0.7240 - acc: 0.7703 - val_loss: 5.1819 - val_acc: 0.4650\n",
      "Epoch 17/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 0.6843 - acc: 0.7799 - val_loss: 5.3059 - val_acc: 0.4668\n",
      "Epoch 18/20\n",
      "5247/5247 [==============================] - 5s 952us/step - loss: 0.6731 - acc: 0.7841 - val_loss: 5.2444 - val_acc: 0.4720\n",
      "Epoch 19/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 0.6217 - acc: 0.7966 - val_loss: 5.2611 - val_acc: 0.4734\n",
      "Epoch 20/20\n",
      "5247/5247 [==============================] - 5s 963us/step - loss: 0.6122 - acc: 0.7995 - val_loss: 5.2812 - val_acc: 0.4738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10954b748>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the network using cross validation (splitting data into training/testing). What is its accuracy? ###\n",
    "\n",
    "**Training and testing on Arial, our model gives us an accuracy of around 47% after 20 epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Conv2D(128, kernel_size=3, activation='relu', input_shape=(20, 20, 1)))\n",
    "model2.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model2.add(Conv2D(128, kernel_size=3, activation='relu'))\n",
    "model2.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dropout(.1))\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dense(arial.unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5247 samples, validate on 20990 samples\n",
      "Epoch 1/20\n",
      "5247/5247 [==============================] - 10s 2ms/step - loss: 6.4524 - acc: 0.1843 - val_loss: 5.7995 - val_acc: 0.2728\n",
      "Epoch 2/20\n",
      "5247/5247 [==============================] - 10s 2ms/step - loss: 4.8385 - acc: 0.3105 - val_loss: 5.4032 - val_acc: 0.3072\n",
      "Epoch 3/20\n",
      "5247/5247 [==============================] - 9s 2ms/step - loss: 3.7011 - acc: 0.3694 - val_loss: 5.4557 - val_acc: 0.3639\n",
      "Epoch 4/20\n",
      "5247/5247 [==============================] - 11s 2ms/step - loss: 2.6890 - acc: 0.4589 - val_loss: 5.3654 - val_acc: 0.3967\n",
      "Epoch 5/20\n",
      "5247/5247 [==============================] - 12s 2ms/step - loss: 1.9660 - acc: 0.5251 - val_loss: 5.3116 - val_acc: 0.4113\n",
      "Epoch 6/20\n",
      "5247/5247 [==============================] - 12s 2ms/step - loss: 1.5409 - acc: 0.5860 - val_loss: 5.2043 - val_acc: 0.4285\n",
      "Epoch 7/20\n",
      "5247/5247 [==============================] - 12s 2ms/step - loss: 1.2694 - acc: 0.6280 - val_loss: 5.2467 - val_acc: 0.4416\n",
      "Epoch 8/20\n",
      "5247/5247 [==============================] - 12s 2ms/step - loss: 1.0805 - acc: 0.6747 - val_loss: 5.2066 - val_acc: 0.4491\n",
      "Epoch 9/20\n",
      "5247/5247 [==============================] - 12s 2ms/step - loss: 0.9625 - acc: 0.7025 - val_loss: 5.1913 - val_acc: 0.4559\n",
      "Epoch 10/20\n",
      "5247/5247 [==============================] - 10s 2ms/step - loss: 0.8788 - acc: 0.7296 - val_loss: 5.3496 - val_acc: 0.4491\n",
      "Epoch 11/20\n",
      "5247/5247 [==============================] - 10s 2ms/step - loss: 0.7897 - acc: 0.7501 - val_loss: 5.3218 - val_acc: 0.4544\n",
      "Epoch 12/20\n",
      "5247/5247 [==============================] - 10s 2ms/step - loss: 0.7307 - acc: 0.7684 - val_loss: 5.3581 - val_acc: 0.4609\n",
      "Epoch 13/20\n",
      "5247/5247 [==============================] - 10s 2ms/step - loss: 0.6810 - acc: 0.7789 - val_loss: 5.3453 - val_acc: 0.4564\n",
      "Epoch 14/20\n",
      "5247/5247 [==============================] - 10s 2ms/step - loss: 0.6609 - acc: 0.7833 - val_loss: 5.3303 - val_acc: 0.4637\n",
      "Epoch 15/20\n",
      "5247/5247 [==============================] - 13s 2ms/step - loss: 0.6094 - acc: 0.7972 - val_loss: 5.3371 - val_acc: 0.4652\n",
      "Epoch 16/20\n",
      "5247/5247 [==============================] - 12s 2ms/step - loss: 0.5822 - acc: 0.8075 - val_loss: 5.4493 - val_acc: 0.4647\n",
      "Epoch 17/20\n",
      "5247/5247 [==============================] - 12s 2ms/step - loss: 0.5567 - acc: 0.8191 - val_loss: 5.3676 - val_acc: 0.4704\n",
      "Epoch 18/20\n",
      "5247/5247 [==============================] - 12s 2ms/step - loss: 0.5404 - acc: 0.8189 - val_loss: 5.4582 - val_acc: 0.4712\n",
      "Epoch 19/20\n",
      "5247/5247 [==============================] - 12s 2ms/step - loss: 0.5098 - acc: 0.8321 - val_loss: 5.4510 - val_acc: 0.4682\n",
      "Epoch 20/20\n",
      "5247/5247 [==============================] - 12s 2ms/step - loss: 0.4947 - acc: 0.8321 - val_loss: 5.3831 - val_acc: 0.4694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1097131d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train a different network topology (add more convolution/dropout layers, explore other types/sizes of layer). Try to find a topology that works better than the one described above. ###\n",
    "\n",
    "**I took a very simple approach with my second model, I just doubled the number of perceptrons of the convolutional and dense layers. This new model gives us a modest increase of accuracy (48% after 20 epochs).  Each epoch took between 2 and 2.5x as long to run though, so it's possible that the increased accuracy would not be worth the time/computing cost in practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Conv2D(64, kernel_size=5, activation='relu', input_shape=(20, 20, 1)))\n",
    "model3.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model3.add(Conv2D(64, kernel_size=5, activation='relu'))\n",
    "model3.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dropout(.1))\n",
    "model3.add(Dense(64, activation='relu'))\n",
    "model3.add(Dense(arial.unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5247 samples, validate on 20990 samples\n",
      "Epoch 1/20\n",
      "5247/5247 [==============================] - 5s 975us/step - loss: 6.7030 - acc: 0.1357 - val_loss: 5.9598 - val_acc: 0.2660\n",
      "Epoch 2/20\n",
      "5247/5247 [==============================] - 4s 841us/step - loss: 5.0567 - acc: 0.2931 - val_loss: 5.4785 - val_acc: 0.2927\n",
      "Epoch 3/20\n",
      "5247/5247 [==============================] - 5s 912us/step - loss: 4.1361 - acc: 0.3307 - val_loss: 5.6373 - val_acc: 0.3394\n",
      "Epoch 4/20\n",
      "5247/5247 [==============================] - 5s 873us/step - loss: 3.3083 - acc: 0.3913 - val_loss: 5.4549 - val_acc: 0.3672\n",
      "Epoch 5/20\n",
      "5247/5247 [==============================] - 5s 896us/step - loss: 2.5561 - acc: 0.4464 - val_loss: 5.2991 - val_acc: 0.3948\n",
      "Epoch 6/20\n",
      "5247/5247 [==============================] - 5s 952us/step - loss: 2.0797 - acc: 0.4904 - val_loss: 5.1689 - val_acc: 0.4126\n",
      "Epoch 7/20\n",
      "5247/5247 [==============================] - 5s 908us/step - loss: 1.7754 - acc: 0.5319 - val_loss: 5.1054 - val_acc: 0.4274\n",
      "Epoch 8/20\n",
      "5247/5247 [==============================] - 4s 849us/step - loss: 1.5728 - acc: 0.5653 - val_loss: 5.0913 - val_acc: 0.4395\n",
      "Epoch 9/20\n",
      "5247/5247 [==============================] - 5s 899us/step - loss: 1.4091 - acc: 0.5965 - val_loss: 5.0866 - val_acc: 0.4420\n",
      "Epoch 10/20\n",
      "5247/5247 [==============================] - 5s 949us/step - loss: 1.2889 - acc: 0.6272 - val_loss: 5.0795 - val_acc: 0.4473\n",
      "Epoch 11/20\n",
      "5247/5247 [==============================] - 5s 983us/step - loss: 1.1974 - acc: 0.6442 - val_loss: 5.0559 - val_acc: 0.4510\n",
      "Epoch 12/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 1.1281 - acc: 0.6688 - val_loss: 5.0075 - val_acc: 0.4542\n",
      "Epoch 13/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 1.0714 - acc: 0.6779 - val_loss: 5.0743 - val_acc: 0.4630\n",
      "Epoch 14/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 1.0012 - acc: 0.6913 - val_loss: 5.0956 - val_acc: 0.4606\n",
      "Epoch 15/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 0.9344 - acc: 0.7155 - val_loss: 5.1279 - val_acc: 0.4566\n",
      "Epoch 16/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 0.8810 - acc: 0.7233 - val_loss: 5.0816 - val_acc: 0.4608\n",
      "Epoch 17/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 0.8632 - acc: 0.7250 - val_loss: 5.1140 - val_acc: 0.4673\n",
      "Epoch 18/20\n",
      "5247/5247 [==============================] - 5s 1ms/step - loss: 0.8144 - acc: 0.7366 - val_loss: 5.0799 - val_acc: 0.4693\n",
      "Epoch 19/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 0.8130 - acc: 0.7450 - val_loss: 5.0919 - val_acc: 0.4701\n",
      "Epoch 20/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 0.7698 - acc: 0.7566 - val_loss: 5.1602 - val_acc: 0.4687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x137175668>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "\n",
    "model4.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(20, 20, 1)))\n",
    "model4.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model4.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model4.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model4.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model4.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model4.add(Flatten())\n",
    "model4.add(Dropout(.1))\n",
    "model4.add(Dense(64, activation='relu'))\n",
    "model4.add(Dense(arial.unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5247 samples, validate on 20990 samples\n",
      "Epoch 1/20\n",
      "5247/5247 [==============================] - 8s 2ms/step - loss: 6.8842 - acc: 0.0661 - val_loss: 6.2954 - val_acc: 0.1274\n",
      "Epoch 2/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 5.5751 - acc: 0.2323 - val_loss: 5.7533 - val_acc: 0.2794\n",
      "Epoch 3/20\n",
      "5247/5247 [==============================] - 5s 1ms/step - loss: 4.7953 - acc: 0.2990 - val_loss: 5.6876 - val_acc: 0.3017\n",
      "Epoch 4/20\n",
      "5247/5247 [==============================] - 5s 958us/step - loss: 4.2420 - acc: 0.3206 - val_loss: 5.6034 - val_acc: 0.3216\n",
      "Epoch 5/20\n",
      "5247/5247 [==============================] - 5s 969us/step - loss: 3.7094 - acc: 0.3469 - val_loss: 5.7690 - val_acc: 0.3398\n",
      "Epoch 6/20\n",
      "5247/5247 [==============================] - 5s 1ms/step - loss: 3.2393 - acc: 0.3806 - val_loss: 5.7219 - val_acc: 0.3510\n",
      "Epoch 7/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 2.8533 - acc: 0.4052 - val_loss: 5.5675 - val_acc: 0.3710\n",
      "Epoch 8/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 2.5131 - acc: 0.4362 - val_loss: 5.5201 - val_acc: 0.3767\n",
      "Epoch 9/20\n",
      "5247/5247 [==============================] - 5s 1ms/step - loss: 2.2976 - acc: 0.4547 - val_loss: 5.4632 - val_acc: 0.3931\n",
      "Epoch 10/20\n",
      "5247/5247 [==============================] - 5s 995us/step - loss: 2.1097 - acc: 0.4770 - val_loss: 5.4378 - val_acc: 0.3978\n",
      "Epoch 11/20\n",
      "5247/5247 [==============================] - 5s 1ms/step - loss: 1.9835 - acc: 0.4999 - val_loss: 5.4677 - val_acc: 0.4001\n",
      "Epoch 12/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 1.8663 - acc: 0.5174 - val_loss: 5.4019 - val_acc: 0.4099\n",
      "Epoch 13/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 1.7082 - acc: 0.5546 - val_loss: 5.4551 - val_acc: 0.4093\n",
      "Epoch 14/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 1.6607 - acc: 0.5571 - val_loss: 5.3884 - val_acc: 0.4111\n",
      "Epoch 15/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 1.5625 - acc: 0.5739 - val_loss: 5.4445 - val_acc: 0.4175\n",
      "Epoch 16/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 1.5055 - acc: 0.5790 - val_loss: 5.3551 - val_acc: 0.4214\n",
      "Epoch 17/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 1.4265 - acc: 0.6089 - val_loss: 5.3882 - val_acc: 0.4262\n",
      "Epoch 18/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 1.3952 - acc: 0.6106 - val_loss: 5.3929 - val_acc: 0.4335\n",
      "Epoch 19/20\n",
      "5247/5247 [==============================] - 7s 1ms/step - loss: 1.3116 - acc: 0.6331 - val_loss: 5.4213 - val_acc: 0.4300\n",
      "Epoch 20/20\n",
      "5247/5247 [==============================] - 6s 1ms/step - loss: 1.2765 - acc: 0.6362 - val_loss: 5.3511 - val_acc: 0.4348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18fe51470>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third and Fourth Models ###\n",
    "\n",
    "**I did a third and forth model mostly for my enjoyment (since the second model was already an improvment over the first).  The third model I changed the kernel size to 5x5 and it underpreformed with 47% accuracy.  The fourth model I added one additional convolutional layer * max pooling layer and (to my surprise) it dramatically underperformed at 43% accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesNewRoman = font('fonts/TIMES.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "0\n",
      "\n",
      "3087\n",
      "\n",
      "(12730, 20, 20, 1)\n",
      "(12730, 3087)\n"
     ]
    }
   ],
   "source": [
    "timesNewRoman.display_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3098 3087\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'intersection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-2b54840487da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreconcileFonts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesNewRoman\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-74b27617abd7>\u001b[0m in \u001b[0;36mreconcileFonts\u001b[0;34m(font1, font2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreconcileFonts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_char_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_char_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfont1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_to_ix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_to_ix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfont1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_to_ix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfont1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_char_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_char_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_to_ix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfont1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_1_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'intersection'"
     ]
    }
   ],
   "source": [
    "reconcileFonts(arial, timesNewRoman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(20, 20, 1)))\n",
    "model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(arial.unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_14 to have shape (3098,) but got array with shape (3087,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d67105eead50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesNewRoman\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesNewRoman\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m                 \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m                 batch_size=batch_size)\n\u001b[0m\u001b[1;32m    973\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0mval_ins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_14 to have shape (3098,) but got array with shape (3087,)"
     ]
    }
   ],
   "source": [
    "model.fit(arial.x, arial.y, validation_data=(timesNewRoman.x, timesNewRoman.y), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the accuracy of your network with character inputs from a DIFFERENT font set. How does it perform? ###\n",
    "\n",
    "**I trained with Arial and tested on Times New Roman**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the accuracy of your network with character inputs from a DIFFERENT font set. How does it perform? (He means train on one font, test on another)\n",
    "\n",
    "Train your best network on inputs from the data from at least 2 different fonts. How does your accuracy compare to the 1-font case? What accuracy do you see when testing with inputs from a font you didn't train on?\n",
    "Take a look at some of the characters that have been misclassified. Do you notice any patterns? The network only produces the relative probabilities that the input is any of the possible characters. Can you find examples where the network is unsure of the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
