{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports and setup \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, KFold\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    x = organize_pixel_values(df)   \n",
    "    y = np.array(df.m_label)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_pixel_values(raw_df):\n",
    "    df = raw_df.copy();\n",
    "    for i in range(12):\n",
    "        df.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "\n",
    "    ret = []\n",
    "    column_vals = df.columns\n",
    "    df_matrix = df.to_numpy()\n",
    "    \n",
    "    for row in range(len(df_matrix)):\n",
    "        pixel_matrix = np.zeros((20, 20))\n",
    "        for col in range(len(df_matrix[0])):\n",
    "            header = column_vals[col]\n",
    "            splitHeader = header[1:].split('c')\n",
    "            pixelRow = splitHeader[0]\n",
    "            pixelCol = splitHeader[1]\n",
    "            pixel_matrix[int(pixelRow)][int(pixelCol)] = df_matrix[row][col] / 255.0\n",
    "        ret.append(pixel_matrix)\n",
    "    \n",
    "    ret = np.stack(ret)\n",
    "    ret = np.reshape(ret, (-1, 20, 20, 1))\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maps_from_labels(input_arr):  \n",
    "    val_to_ix = { val:i for i,val in enumerate(np.unique(input_arr)) }\n",
    "    ix_to_val = { i:val for i,val in enumerate(np.unique(input_arr)) }\n",
    "    \n",
    "    return val_to_ix, ix_to_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconcile_fonts(font1, font2):\n",
    "    x1 = font1.x\n",
    "    y1 = font1.y_raw\n",
    "    x2 = font2.x\n",
    "    y2 = font2.y_raw\n",
    "    \n",
    "#     print(len(x1))\n",
    "#     print(len(y1))\n",
    "#     print(len(x2))\n",
    "#     print(len(y2))\n",
    "\n",
    "    intersection = np.intersect1d(y1, y2, assume_unique=False, return_indices=False)\n",
    "    \n",
    "#     print(intersection)\n",
    "    \n",
    "    x1_fixed = []\n",
    "    y1_fixed = []\n",
    "    for i in range(len(y1)):\n",
    "        if (y1[i] in intersection):\n",
    "            x1_fixed.append(x1[i])\n",
    "            y1_fixed.append(y1[i])\n",
    "\n",
    "    x2_fixed = []\n",
    "    y2_fixed = []\n",
    "    for i in range(len(y2)):\n",
    "        if (y2[i] in intersection):\n",
    "            x2_fixed.append(x2[i])\n",
    "            y2_fixed.append(y2[i])\n",
    "    \n",
    "#     print(len(x1_fixed))\n",
    "#     print(len(y1_fixed))\n",
    "#     print(len(x2_fixed))\n",
    "#     print(len(y2_fixed))\n",
    "    \n",
    "    font1_fixed = Font(x1_fixed, y1_fixed)\n",
    "    font2_fixed = Font(x2_fixed, y2_fixed)\n",
    "    \n",
    "#     print(font1.unique_char_count)\n",
    "#     print(font1.unique_char_count)\n",
    "    \n",
    "    return font1_fixed, font2_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Font:\n",
    "    x = None\n",
    "    y = None\n",
    "    y_raw = None\n",
    "    \n",
    "    unique_char_count = 0\n",
    "    val_to_ix = None\n",
    "    ix_to_val = None\n",
    "    \n",
    "    def __init__(self, data, labels):\n",
    "        self.x = data\n",
    "        self.y_raw = labels\n",
    "        self.val_to_ix, self.ix_to_val = get_maps_from_labels(self.y_raw)\n",
    "        self.unique_char_count = len(self.val_to_ix)\n",
    "        \n",
    "        self.y = self.get_1_hot()\n",
    "        \n",
    "    def get_1_hot(self):\n",
    "        ret = []\n",
    "        for val in self.y_raw:\n",
    "            arr = np.zeros(self.unique_char_count)\n",
    "            arr[self.val_to_ix[val]] = 1;\n",
    "            ret.append(arr)\n",
    "\n",
    "        return np.array(ret)\n",
    "    \n",
    "    def display_attributes(self):\n",
    "        print(self.ix_to_val[0])\n",
    "        print(self.val_to_ix[33])\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(self.unique_char_count)\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(self.x.shape)\n",
    "        print(self.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_data_from_file('fonts/ARIAL.csv')\n",
    "arial = Font(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "0\n",
      "\n",
      "3098\n",
      "\n",
      "(26237, 20, 20, 1)\n",
      "(26237, 3098)\n"
     ]
    }
   ],
   "source": [
    "arial.display_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(arial.x, arial.y, random_state=1, test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(20, 20, 1)))\n",
    "model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(arial.unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the network using cross validation (splitting data into training/testing). What is its accuracy? ###\n",
    "\n",
    "**Training and testing on Arial, our model gives us an accuracy of around 47% after 20 epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Conv2D(128, kernel_size=3, activation='relu', input_shape=(20, 20, 1)))\n",
    "model2.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model2.add(Conv2D(128, kernel_size=3, activation='relu'))\n",
    "model2.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dropout(.1))\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dense(arial.unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train a different network topology (add more convolution/dropout layers, explore other types/sizes of layer). Try to find a topology that works better than the one described above. ###\n",
    "\n",
    "**I took a very simple approach with my second model, I just doubled the number of perceptrons of the convolutional and dense layers. This new model gives us a modest increase of accuracy (48% after 20 epochs).  Each epoch took between 2 and 2.5x as long to run though, so it's possible that the increased accuracy would not be worth the time/computing cost in practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Conv2D(64, kernel_size=5, activation='relu', input_shape=(20, 20, 1)))\n",
    "model3.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model3.add(Conv2D(64, kernel_size=5, activation='relu'))\n",
    "model3.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dropout(.1))\n",
    "model3.add(Dense(64, activation='relu'))\n",
    "model3.add(Dense(arial.unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "\n",
    "model4.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(20, 20, 1)))\n",
    "model4.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model4.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model4.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model4.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model4.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model4.add(Flatten())\n",
    "model4.add(Dropout(.1))\n",
    "model4.add(Dense(64, activation='relu'))\n",
    "model4.add(Dense(arial.unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third and Fourth Models ###\n",
    "\n",
    "**I did a third and forth model mostly for my enjoyment (since the second model was already an improvment over the first).  The third model I changed the kernel size to 5x5 and it underpreformed with 47% accuracy.  The fourth model I added one additional convolutional layer * max pooling layer and (to my surprise) it dramatically underperformed at 43% accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_data_from_file('fonts/TIMES.csv')\n",
    "times = Font(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "0\n",
      "\n",
      "3087\n",
      "\n",
      "(12730, 20, 20, 1)\n",
      "(12730, 3087)\n"
     ]
    }
   ],
   "source": [
    "times.display_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedArial, fixedTimes = reconcile_fonts(arial, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(20, 20, 1)))\n",
    "model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(arial.unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(arial.x, arial.y, validation_data=(timesNewRoman.x, timesNewRoman.y), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the accuracy of your network with character inputs from a DIFFERENT font set. How does it perform? ###\n",
    "\n",
    "**I trained with Arial and tested on Times New Roman**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the accuracy of your network with character inputs from a DIFFERENT font set. How does it perform? (He means train on one font, test on another)\n",
    "\n",
    "Train your best network on inputs from the data from at least 2 different fonts. How does your accuracy compare to the 1-font case? What accuracy do you see when testing with inputs from a font you didn't train on?\n",
    "Take a look at some of the characters that have been misclassified. Do you notice any patterns? The network only produces the relative probabilities that the input is any of the possible characters. Can you find examples where the network is unsure of the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
