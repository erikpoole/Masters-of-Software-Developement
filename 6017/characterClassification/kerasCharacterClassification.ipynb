{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports and setup \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, KFold\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    x = organize_pixel_values(df)   \n",
    "    y = np.array(df.m_label)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_pixel_values(raw_df):\n",
    "    df = raw_df.copy();\n",
    "    for i in range(12):\n",
    "        df.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "\n",
    "    ret = []\n",
    "    column_vals = df.columns\n",
    "    df_matrix = df.to_numpy()\n",
    "    \n",
    "    for row in range(len(df_matrix)):\n",
    "        pixel_matrix = np.zeros((20, 20))\n",
    "        for col in range(len(df_matrix[0])):\n",
    "            header = column_vals[col]\n",
    "            splitHeader = header[1:].split('c')\n",
    "            pixelRow = splitHeader[0]\n",
    "            pixelCol = splitHeader[1]\n",
    "            pixel_matrix[int(pixelRow)][int(pixelCol)] = df_matrix[row][col] / 255.0\n",
    "        ret.append(pixel_matrix)\n",
    "    \n",
    "    ret = np.stack(ret)\n",
    "    ret = np.reshape(ret, (-1, 20, 20, 1))\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_maps_from_labels(input_arr):  \n",
    "    val_to_ix = { val:i for i,val in enumerate(np.unique(input_arr)) }\n",
    "    ix_to_val = { i:val for i,val in enumerate(np.unique(input_arr)) }\n",
    "    \n",
    "    return val_to_ix, ix_to_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconcile_fonts(font1, font2):\n",
    "    x1 = font1.x\n",
    "    y1 = font1.y_raw\n",
    "    x2 = font2.x\n",
    "    y2 = font2.y_raw\n",
    "    \n",
    "#     print(len(x1))\n",
    "#     print(len(y1))\n",
    "#     print(len(x2))\n",
    "#     print(len(y2))\n",
    "\n",
    "    intersection = np.intersect1d(y1, y2, assume_unique=False, return_indices=False)\n",
    "    \n",
    "#     print(intersection)\n",
    "    \n",
    "    x1_fixed = []\n",
    "    y1_fixed = []\n",
    "    for i in range(len(y1)):\n",
    "        if (y1[i] in intersection):\n",
    "            x1_fixed.append(x1[i])\n",
    "            y1_fixed.append(y1[i])\n",
    "\n",
    "    x2_fixed = []\n",
    "    y2_fixed = []\n",
    "    for i in range(len(y2)):\n",
    "        if (y2[i] in intersection):\n",
    "            x2_fixed.append(x2[i])\n",
    "            y2_fixed.append(y2[i])\n",
    "    \n",
    "#     print(len(x1_fixed))\n",
    "#     print(len(y1_fixed))\n",
    "#     print(len(x2_fixed))\n",
    "#     print(len(y2_fixed))\n",
    "    \n",
    "    font1_fixed = Font(np.asarray(x1_fixed), np.asarray(y1_fixed))\n",
    "    font2_fixed = Font(np.asarray(x2_fixed), np.asarray(y2_fixed))\n",
    "    \n",
    "#     print(font1.unique_char_count)\n",
    "#     print(font1.unique_char_count)\n",
    "    \n",
    "    return font1_fixed, font2_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Font:\n",
    "    x = None\n",
    "    y = None\n",
    "    y_raw = None\n",
    "    \n",
    "    unique_char_count = 0\n",
    "    val_to_ix = None\n",
    "    ix_to_val = None\n",
    "    \n",
    "    def __init__(self, data, labels):\n",
    "        self.x = data\n",
    "        self.y_raw = labels\n",
    "        self.val_to_ix, self.ix_to_val = get_maps_from_labels(self.y_raw)\n",
    "        self.unique_char_count = len(self.val_to_ix)\n",
    "        \n",
    "        self.y = self.get_1_hot()\n",
    "        \n",
    "    def get_1_hot(self):\n",
    "        ret = []\n",
    "        for val in self.y_raw:\n",
    "            arr = np.zeros(self.unique_char_count)\n",
    "            arr[self.val_to_ix[val]] = 1;\n",
    "            ret.append(arr)\n",
    "\n",
    "        return np.array(ret)\n",
    "    \n",
    "    def display_attributes(self):\n",
    "        print(self.ix_to_val[0])\n",
    "        print(self.val_to_ix[33])\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(self.unique_char_count)\n",
    "\n",
    "        print()\n",
    "\n",
    "        print(self.x.shape)\n",
    "        print(self.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_data_from_file('fonts/ARIAL.csv')\n",
    "arial = Font(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "0\n",
      "\n",
      "3098\n",
      "\n",
      "(26237, 20, 20, 1)\n",
      "(26237, 3098)\n"
     ]
    }
   ],
   "source": [
    "arial.display_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(arial.x, arial.y, random_state=1, test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(20, 20, 1)))\n",
    "model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(arial.unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the network using cross validation (splitting data into training/testing). What is its accuracy? ###\n",
    "\n",
    "**Training and testing on Arial, our model gives us an accuracy of around 47% after 20 epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Conv2D(128, kernel_size=3, activation='relu', input_shape=(20, 20, 1)))\n",
    "model2.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model2.add(Conv2D(128, kernel_size=3, activation='relu'))\n",
    "model2.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dropout(.1))\n",
    "model2.add(Dense(128, activation='relu'))\n",
    "model2.add(Dense(arial.unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train a different network topology (add more convolution/dropout layers, explore other types/sizes of layer). Try to find a topology that works better than the one described above. ###\n",
    "\n",
    "**I took a very simple approach with my second model, I just doubled the number of perceptrons of the convolutional and dense layers. This new model gives us a modest increase of accuracy (48% after 20 epochs).  Each epoch took between 2 and 2.5x as long to run though, so it's possible that the increased accuracy would not be worth the time/computing cost in practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Conv2D(64, kernel_size=5, activation='relu', input_shape=(20, 20, 1)))\n",
    "model3.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model3.add(Conv2D(64, kernel_size=5, activation='relu'))\n",
    "model3.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dropout(.1))\n",
    "model3.add(Dense(64, activation='relu'))\n",
    "model3.add(Dense(arial.unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "\n",
    "model4.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(20, 20, 1)))\n",
    "model4.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model4.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model4.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model4.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model4.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model4.add(Flatten())\n",
    "model4.add(Dropout(.1))\n",
    "model4.add(Dense(64, activation='relu'))\n",
    "model4.add(Dense(arial.unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third and Fourth Models ###\n",
    "\n",
    "**I did a third and forth model mostly for my enjoyment (since the second model was already an improvment over the first).  The third model I changed the kernel size to 5x5 and it underpreformed with 47% accuracy.  The fourth model I added one additional convolutional layer * max pooling layer and (to my surprise) it dramatically underperformed at 43% accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_data_from_file('fonts/TIMES.csv')\n",
    "times = Font(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "0\n",
      "\n",
      "3087\n",
      "\n",
      "(12730, 20, 20, 1)\n",
      "(12730, 3087)\n"
     ]
    }
   ],
   "source": [
    "times.display_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedArial, fixedTimes = reconcile_fonts(arial, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "0\n",
      "\n",
      "3087\n",
      "\n",
      "(26180, 20, 20, 1)\n",
      "(26180, 3087)\n"
     ]
    }
   ],
   "source": [
    "fixedArial.display_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "0\n",
      "\n",
      "3087\n",
      "\n",
      "(12730, 20, 20, 1)\n",
      "(12730, 3087)\n"
     ]
    }
   ],
   "source": [
    "fixedTimes.display_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_font_comparison = Sequential()\n",
    "\n",
    "model_font_comparison.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(20, 20, 1)))\n",
    "model_font_comparison.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model_font_comparison.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model_font_comparison.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model_font_comparison.add(Flatten())\n",
    "model_font_comparison.add(Dropout(.1))\n",
    "model_font_comparison.add(Dense(64, activation='relu'))\n",
    "model_font_comparison.add(Dense(fixedArial.unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_font_comparison.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_font_comparison.fit(fixedArial.x, fixedArial.y, validation_data=(fixedTimes.x, fixedTimes.y), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the accuracy of your network with character inputs from a DIFFERENT font set. How does it perform? ###\n",
    "\n",
    "**I trained with Arial and tested on Times New Roman and, unsurprisingly we took fairly significant hit ot our accuracy - down all the way to 31%.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_data_from_file('fonts/COMIC.csv')\n",
    "comic_sans = Font(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "0\n",
      "\n",
      "597\n",
      "\n",
      "(2388, 20, 20, 1)\n",
      "(2388, 597)\n"
     ]
    }
   ],
   "source": [
    "comic_sans.display_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedArial, fixedComic = reconcile_fonts(fixedArial, comic_sans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixedTimes, fixedComic = reconcile_fonts(fixedTimes, fixedComic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "0\n",
      "\n",
      "579\n",
      "\n",
      "(16872, 20, 20, 1)\n",
      "(16872, 579)\n"
     ]
    }
   ],
   "source": [
    "fixedArial.display_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "0\n",
      "\n",
      "579\n",
      "\n",
      "(4034, 20, 20, 1)\n",
      "(4034, 579)\n"
     ]
    }
   ],
   "source": [
    "fixedTimes.display_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "0\n",
      "\n",
      "579\n",
      "\n",
      "(2316, 20, 20, 1)\n",
      "(2316, 579)\n"
     ]
    }
   ],
   "source": [
    "fixedComic.display_attributes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0717 09:22:06.313472 4551812544 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0717 09:22:06.360015 4551812544 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0717 09:22:06.374168 4551812544 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0717 09:22:06.407280 4551812544 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0717 09:22:06.444809 4551812544 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0717 09:22:06.452932 4551812544 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model_2fonts = Sequential()\n",
    "\n",
    "model_2fonts.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(20, 20, 1)))\n",
    "model_2fonts.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model_2fonts.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model_2fonts.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
    "model_2fonts.add(Flatten())\n",
    "model_2fonts.add(Dropout(.1))\n",
    "model_2fonts.add(Dense(64, activation='relu'))\n",
    "model_2fonts.add(Dense(fixedArial.unique_char_count, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0717 09:22:06.523973 4551812544 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0717 09:22:06.555028 4551812544 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_2fonts.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0717 09:22:06.840777 4551812544 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20906 samples, validate on 2316 samples\n",
      "Epoch 1/20\n",
      "20906/20906 [==============================] - 11s 542us/step - loss: 3.0091 - acc: 0.4680 - val_loss: 3.8140 - val_acc: 0.2396\n",
      "Epoch 2/20\n",
      "20906/20906 [==============================] - 9s 451us/step - loss: 1.1780 - acc: 0.6721 - val_loss: 3.2768 - val_acc: 0.3169\n",
      "Epoch 3/20\n",
      "20906/20906 [==============================] - 9s 436us/step - loss: 0.8557 - acc: 0.7363 - val_loss: 3.2978 - val_acc: 0.3342\n",
      "Epoch 4/20\n",
      "20906/20906 [==============================] - 9s 438us/step - loss: 0.7106 - acc: 0.7681 - val_loss: 3.2444 - val_acc: 0.3605\n",
      "Epoch 5/20\n",
      "20906/20906 [==============================] - 9s 444us/step - loss: 0.6114 - acc: 0.7952 - val_loss: 3.3230 - val_acc: 0.3692\n",
      "Epoch 6/20\n",
      "20906/20906 [==============================] - 9s 445us/step - loss: 0.5451 - acc: 0.8145 - val_loss: 3.2099 - val_acc: 0.3916\n",
      "Epoch 7/20\n",
      "20906/20906 [==============================] - 12s 569us/step - loss: 0.4953 - acc: 0.8275 - val_loss: 3.4517 - val_acc: 0.3864\n",
      "Epoch 8/20\n",
      "20906/20906 [==============================] - 12s 575us/step - loss: 0.4547 - acc: 0.8416 - val_loss: 3.5658 - val_acc: 0.3934\n",
      "Epoch 9/20\n",
      "20906/20906 [==============================] - 12s 588us/step - loss: 0.4200 - acc: 0.8493 - val_loss: 3.2817 - val_acc: 0.4197\n",
      "Epoch 10/20\n",
      "20906/20906 [==============================] - 12s 584us/step - loss: 0.3883 - acc: 0.8598 - val_loss: 3.5136 - val_acc: 0.4085\n",
      "Epoch 11/20\n",
      "20906/20906 [==============================] - 12s 596us/step - loss: 0.3707 - acc: 0.8625 - val_loss: 3.6016 - val_acc: 0.4218\n",
      "Epoch 12/20\n",
      "20906/20906 [==============================] - 12s 584us/step - loss: 0.3544 - acc: 0.8733 - val_loss: 3.3807 - val_acc: 0.4344\n",
      "Epoch 13/20\n",
      "20906/20906 [==============================] - 12s 589us/step - loss: 0.3314 - acc: 0.8787 - val_loss: 3.7113 - val_acc: 0.4214\n",
      "Epoch 14/20\n",
      "20906/20906 [==============================] - 13s 606us/step - loss: 0.3260 - acc: 0.8766 - val_loss: 3.7302 - val_acc: 0.4080\n",
      "Epoch 15/20\n",
      "20906/20906 [==============================] - 10s 495us/step - loss: 0.3051 - acc: 0.8845 - val_loss: 3.5742 - val_acc: 0.4288\n",
      "Epoch 16/20\n",
      "20906/20906 [==============================] - 10s 460us/step - loss: 0.2912 - acc: 0.8867 - val_loss: 3.5769 - val_acc: 0.4478\n",
      "Epoch 17/20\n",
      "20906/20906 [==============================] - 11s 546us/step - loss: 0.2911 - acc: 0.8893 - val_loss: 3.6814 - val_acc: 0.4417\n",
      "Epoch 18/20\n",
      "20906/20906 [==============================] - 12s 579us/step - loss: 0.2801 - acc: 0.8933 - val_loss: 3.7463 - val_acc: 0.4378\n",
      "Epoch 19/20\n",
      "20906/20906 [==============================] - 12s 595us/step - loss: 0.2766 - acc: 0.8958 - val_loss: 3.8449 - val_acc: 0.4326\n",
      "Epoch 20/20\n",
      "20906/20906 [==============================] - 12s 580us/step - loss: 0.2619 - acc: 0.8996 - val_loss: 3.7592 - val_acc: 0.4400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x102cac4a8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2fonts.fit(np.concatenate([fixedArial.x, fixedTimes.x]), \\\n",
    "                 np.concatenate([fixedArial.y, fixedTimes.y]), \\\n",
    "                 validation_data=(fixedComic.x, fixedComic.y), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your best network on inputs from the data from at least 2 different fonts. How does your accuracy compare to the 1-font case? What accuracy do you see when testing with inputs from a font you didn't train on?###\n",
    "\n",
    "**I trained with Times New Roman and Arial and tested on Comic Sans.  The accuracy was dramatically better than our previous training on Times and testing on Arial (44% instead of 31%).  This might be because two datasets combined for training does a better job but it might also be because the comic sans dataset was much smaller than the previous two and so there are far fewer potential outputs (597 instead of 3087)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the accuracy of your network with character inputs from a DIFFERENT font set. How does it perform? (He means train on one font, test on another)\n",
    "\n",
    "Train your best network on inputs from the data from at least 2 different fonts. How does your accuracy compare to the 1-font case? What accuracy do you see when testing with inputs from a font you didn't train on?\n",
    "\n",
    "Take a look at some of the characters that have been misclassified. Do you notice any patterns? The network only produces the relative probabilities that the input is any of the possible characters. Can you find examples where the network is unsure of the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
